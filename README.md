
## 游꺕 Code Structure

```
較럭MVLPNet
|   較럭utils.py
|   較럭vis.py # vis.py is the code for visualization
|   較럭test.py
|   較럭test.sh
|   較럭train.py
|   較럭train.sh
|   較럭train_base.py
|   較럭train_base.sh
|   較럭util
|   較럭model
|   |   較럭workdir
|   |   較럭util
|   |   較럭few_seg
|   |   |    較MVLPNet.py
|   |   較럭backbone
|   |   較럭clip
|   較럭lists
|   較럭initmodel
|   |     較럭PSPNet
|   |     較럭CLIP
|   較럭vgg16_bn.pth
|   較럭resnet50_v2.pth
|   較럭exp
|   較럭dataset
|   較럭config
較럭data
|  較럭iSAID
|  |   較럭train.txt
|  |   較럭val.txt
|  |   較럭img_dir
|  |   較럭ann_dir
```

## 游닇 Data Preparation

- Create a folder `data` at the same level as this repo in the root directory.

  ```
  cd ..
  mkdir data
  ```
- iSAID_512:
iSAID.tar.gz : https://pan.baidu.com/s/11ZhZ01KVjfPyHcoZ2MkfeA password: 0000

- iSAID_256:
iSAID.tar.gz : https://pan.baidu.com/s/1WgZBH075gjmypS4NbiLaXg password: 0000 

- LoveDA:
LoveDA.tar.gz : https://pan.baidu.com/s/1XG7zsh5uTOerffrE73cj2g password: 0000 

## Train

### Training base-learners (two options)

- Option 1: training from scratch

  Download the pre-trained backbones from (https://pan.baidu.com/s/1tWAUKYvP-sh_LcCOy1-P7Q password: 0000) and put them into the `MVLPNet/initmodel` directory.
  The clip model is placed in the `MVLPNet/initmodel` directory: (https://pan.baidu.com/s/1vwtIinePOP7UdhrEDj4HKg password: 0000)
  ```
  sh train_base.sh
  ```
- Option 2: loading the trained models
  
  ```
  mkdir initmodel
  cd initmodel
  ```
  
  Put the provided (https://pan.baidu.com/s/1I4s8PLy4N5Qb7UeE7VsVXQ password: 0000) in the newly created folder `initmodel` and rename the downloaded file to `PSPNet`, *i.e.*, `MVLPNet/initmodel/PSPNet`.

### Training few-shot models

To train a model, run

```
sh train.sh
```

### Testing few-shot models

To evaluate the trained models, run

```
sh test.sh
```


## 游녪 Acknowledgements
The project is based on [PFENet](https://github.com/dvlab-research/PFENet) , [R2Net](https://github.com/chunbolang/R2Net) and [PI-CLIP](https://github.com/vangjin/PI-CLIP). Thanks for the authors for their efforts.

